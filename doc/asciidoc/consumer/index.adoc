[[consumer]]
== Consumer: Kafka -> Neo4j

ifdef::env-docs[]
[abstract]
--
This chapter describes the Neo4j Streams Consumer in the Neo4j Streams Library. Use this section
to configure Neo4j to consume data from Kafka and create new nodes and relationships.
--
endif::env-docs[]

Is the Kafka Sink that ingest the data directly into Neo4j

=== How it works

It works in two ways:

* by providing a Cypher template
* by ingesting the events emitted from another Neo4j instance via the Change Data Capture module

==== Cypher Template

It works with template Cypher queries stored into properties with the following format:

[source,ini]
----
streams.sink.topic.cypher.<TOPIC_NAME>=<CYPHER_QUERY>
----

Each Cypher template must refer to an *event* object that will be injected by the Sink

Following an example:

For this event

[source,javascript]
----
{
 "id": 42,
 "properties": {
   "title": "Answer to anyting",
   "description": "It depends."
 }
}
----

.neo4j.conf
[source,ini]
----
streams.sink.topic.cypher.my-topic=MERGE (n:Label {id: event.id}) \
    ON CREATE SET n += event.properties
----

Under the hood the Sink inject the event object as a parameter in this way

[source,cypher]
----
UNWIND {events} AS event
MERGE (n:Label {id: event.id})
    ON CREATE SET n += event.properties
----

Where `{batch}` is a json list, so continuing with the example above a possible full representation could be:

[source,cypher]
----
:params events => [{id:"alice@example.com",properties:{name:"Alice",age:32}},
    {id:"bob@example.com",properties:{name:"Bob",age:42}}]

UNWIND {events} AS event
MERGE (n:Label {id: event.id})
    ON CREATE SET n += event.properties
----

==== Change Data Capture Event

This method allows to ingest CDC events coming from another Neo4j Instance. You can use two strategies:

 * The `SourceId` strategy which merges the nodes/relationships by the CDC event `id` field (it's related to the Neo4j physical ID)
 * The `Schema` strategy which merges the nodes/relationships by the constraints (UNIQUENESS, NODE_KEY) defined in your graph model

===== The `SourceId` strategy

You can configure the topic in the following way:

[source,ini]
----
streams.sink.topic.cdc.sourceId=<list of topics separated by semicolon>
streams.sink.topic.cdc.sourceId.labelName=<the label attached to the node, default=SourceEvent>
streams.sink.topic.cdc.sourceId.idName=<the id name given to the CDC id field, default=sourceId>
----

[source,ini]
----
streams.sink.topic.cdc.sourceId=my-topic;my-other.topic
----

Each streams event will be projected into the related graph entity, for instance the following event:

[source,json]
----
include::../producer/data/node.created.json[]
----

will be persisted as the following node:

```
Person:SourceEvent{first_name: "Anne Marie", last_name: "Kretchmar", email: "annek@noanswer.org", sourceId: "1004"}
```

as you can notice, ingested event has been projected with two peculiarities:

* the `id` field has transformed into `sourceId`;
* the node has an additional label `SourceEvent`;

these two fields will be used in order to match the node/relationship for future updates/deletes

===== The `schema` strategy

You can configure the topic in the following way:

[source,ini]
----
streams.sink.topic.cdc.schema=<LIST_OF_TOPICS_SEPARATE_BY_SEMICOLON>
----

[source,ini]
----
streams.sink.topic.cdc.schema=my-topic;my-other.topic
----

Each streams event will be projected into the related graph entity, for instance the following event:

[source,json]
----
include::../producer/data/node.created.json[]
----

will be persisted as the following node:

```
Person{first_name: "Anne Marie", last_name: "Kretchmar", email: "annek@noanswer.org"}
```

The `Schema` strategy leverages the `schema` field in order to insert/update the nodes so no extra fields will be created.

In case of relationship

[source,json]
----
include::../producer/data/relationship.created.json[]
----

the `Schema` strategy leverages the `ids` fields in order to insert/update the relationships so no extra fields will be created.

===== The `Pattern` strategy

The `Pattern` strategy allows you to extract nodes and relationships from a json by providing a extraction pattern

Each property can be prefixed with:

* `!`: identify the id (could be more than one property), it's *mandatory*
* `-`: exclude the property from the extraction
If no prefix is specified this means that the property will be included

*Note*: you cannot mix inclusion and exclusion so if your pattern must contains all exclusion
or inclusion properties

Labels can be chained via `:`

.Tombstone Record Management
The pattern strategy come out with the support to the https://en.wikipedia.org/wiki/Tombstone_(data_store)[Tombstone Record],
in order to leverage it your event should contain as key the record that you want to delete and `null` for the value.

====== The `Node Pattern` configuration

You can configure the node pattern extraction in the following way:

```
streams.sink.topic.pattern.node.<TOPIC_NAME>=<NODE_EXTRACTION_PATTERN>
```

So for instance, given the following `json` published via the `user` topic:

[source,json]
----
{"userId": 1, "name": "Andrea", "surname": "Santurbano", "address": {"city": "Venice", "cap": "30100"}}
----

You can transform it into a node by providing the following configuration:


by specifying a simpler pattern:

```
streams.sink.topic.pattern.node.user=User{!userId}
```

or by specifying a Cypher like node pattern:

```
streams.sink.topic.pattern.node.user=(:User{!userId})
```

Similar to the CDC pattern you can provide:

[cols="1m,3a",opts=header]
|===
| pattern
| meaning

| User:Actor{!userId} or User:Actor{!userId,*}
| the userId will be used as ID field and all properties of the json will be attached to the node with the provided
labels (`User` and `Actor`) so the persisted node will be: *(User:Actor{userId: 1, name: 'Andrea', surname: 'Santurbano', `address.city`: 'Venice', `address.cap`: 30100})*

| User{!userId, surname}
| the userId will be used as ID field and *only* the surname property of the json will be attached to the node with the provided
labels (`User`) so the persisted node will be: *(User:Actor{userId: 1, surname: 'Santurbano'})*

| User{!userId, surname, address.city}
| the userId will be used as ID field and *only* the surname and the `address.city` property of the json will be attached to the node with the provided
labels (`User`) so the persisted node will be: *(User:Actor{userId: 1, surname: 'Santurbano', `address.city`: 'Venice'})*

| User{!userId,-address}
| the userId will be used as ID field and the `address` property will be excluded
so the persisted node will be: *(User:Actor{userId: 1, name: 'Andrea', surname: 'Santurbano'})*

|===

====== The `Relationship Pattern` configuration

You can configure the relationship pattern extraction in the following way:

```
streams.sink.topic.pattern.relationship.<TOPIC_NAME>=<RELATIONSHIP_EXTRACTION_PATTERN>
```

So for instance, given the following `json` published via the `user` topic:

[source,json]
----
{"userId": 1, "productId": 100, "price": 10, "currency": "€", "shippingAddress": {"city": "Venice", cap: "30100"}}
----

You can transform it into a node by providing the following configuration:

By specifying a simpler pattern:

```
streams.sink.topic.pattern.relationship.user=User{!userId} BOUGHT{price, currency} Product{!productId}
```

or by specifying a Cypher like node pattern:

```
streams.sink.topic.pattern.relationship.user=(:User{!userId})-[:BOUGHT{price, currency}]->(:Product{!productId})
```

in this last case the we assume that `User` is the source node and `Product` the target node


Similar to the CDC pattern you can provide:

[cols="1m,3a",opts=header]
|===
| pattern
| meaning

| (User{!userId})-[:BOUGHT]->(Product{!productId}) or (User{!userId})-[:BOUGHT{price, currency}]->(Product{!productId})
| this will merge fetch/create the two nodes by the provided identifier and the `BOUGHT` relationship between them. And then set all the other json properties on them so the persisted data will be:
*(User{userId: 1})-[:BOUGHT{price: 10, currency: '€', `shippingAddress.city`: 'Venice', `shippingAddress.cap`: 30100}]->(Product{productId: 100})*

| (User{!userId})-[:BOUGHT{price}]->(Product{!productId})
| this will merge fetch/create the two nodes by the provided identifier and the `BOUGHT` relationship between them. And then set all the specified json properties so the persisted pattern will be:
*(User{userId: 1})-[:BOUGHT{price: 10}]->(Product{productId: 100})*

| (User{!userId})-[:BOUGHT{-shippingAddress}]->(Product{!productId})
| this will merge fetch/create the two nodes by the provided identifier and the `BOUGHT` relationship between them. And then set all the specified json properties (by the exclusion) so the persisted pattern will be:
*(User{userId: 1})-[:BOUGHT{price: 10, currency: '€'}]->(Product{productId: 100})*

| (User{!userId})-[:BOUGHT{price,currency, shippingAddress.city}]->(Product{!productId})
| this will merge fetch/create the two nodes by the provided identifier and the `BOUGHT` relationship between them. And then set all the specified json properties so the persisted pattern will be:
*(User{userId: 1})-[:BOUGHT{price: 10, currency: '€', `shippingAddress.city`: 'Venice'}]->(Product{productId: 100})*

|===


*Attach properties to node*

By default no properties will be attached to the edge nodes but you can specify which property attach to each node. Given the following `json` published via the `user` topic:

[source,json]
----
{
    "userId": 1,
    "userName": "Andrea",
    "userSurname": "Santurbano",
    "productId": 100,
    "productName": "My Awesome Product!",
    "price": 10,
    "currency": "€"
}
----

[cols="1m,3a",opts=header]
|===
| pattern
| meaning

| (User{!userId, userName, userSurname})-[:BOUGHT]->(Product{!productId, productName})
| this will merge two nodes and the `BOUGHT` relationship between with all json properties them so the persisted pattern will be:
*(User{userId: 1, userName: 'Andrea', userSurname: 'Santurbano'})-[:BOUGHT{price: 10, currency: '€'}]->(Product{productId: 100, name: 'My Awesome Product!'})*

|===

=== How deal with bad data

The Neo4j Streams Plugin provides a way to re-route all the data that for something reason it wasn't able to ingest to a `Dead Letter Queue`.
You can active the feature by setting the following property:

```
streams.sink.dlq=<TOPIC_NAME>
```

So given:

```
streams.sink.dlq=my-dlq-topic
```

the Neo4j Streams plugin will re-route all the data to the the `my-dlq-topic` topic.

Every published record in the `Dead Letter Queue` contains the original record `Key` and `Value` pairs and the following headers:

[cols="1m,3a",opts=header]
|===
| Header key
| Description

| "__streams.errorstopic"
| The topic where the data is published

| "__streams.errorspartition"
| The topic partition where the data is published

| "__streams.errorsoffset"
| The offset of the data into the topic partition

| "__streams.errorsclass.name"
| The class that generated the error

| "__streams.errorsexception.class.name
| The exception that generated the error

| "__streams.errorsexception.message"
| The exception message

| "__streams.errorsexception.stacktrace"
| The exception stack trace
|===


=== Configuration

include::configuration.adoc[]
