= Neo4j Streaming Data Integrations User Guide v{docs-version}
:toc: left
:experimental:
:toclevels: 2
:sectid:
:sectlinks:
:img: https://github.com/neo4j-contrib/neo4j-streams/raw/gh-pages/3.4/images
:env-docs: true

ifdef::backend-html5[(C) {copyright}]

License: link:{common-license-page-uri}[Creative Commons 4.0]


[abstract]
--
This is the user guide for Neo4j Streams version {docs-version}, authored by the Neo4j Labs Team.
--

The guide covers the following areas:

* <<quickstart>> -- Get Started Fast with the most Common Scenarios
* <<introduction>> -- An introduction to Neo4j Streams
* <<producer>> -- Sends transaction event handler events to a Kafka topic
* <<consumer>> -- Ingests events from a Kafka topic into Neo4j
* <<procedures>> -- Procedures for consuming and producing Kafka events
* <<docker>> -- Docker Compose files for local testing
* <<kafka-connect>> -- Kafka Connect Sink plugin

[[quickstart]]
== Quick Start

ifdef::env-docs[]
[abstract]
--
Get started fast for common scenarios, using neo4j-streams as a plugin.
--
endif::env-docs[]

1. Download the latest release jar from https://github.com/neo4j-contrib/neo4j-streams/releases/latest
2. Copy it into `$NEO4J_HOME/plugins` and configure the relevant connections
3. Configure the plugin to connect to your kafka instance.

If you are managing Kafka yourself, the simple setup is as follows:

----
kafka.zookeeper.connect=localhost:2181
kafka.bootstrap.servers=localhost:9092
----

If you are using Confluent Cloud (managed Kafka), you can connect to Kafka in this way, filling 
in your own `CONFLUENT_CLOUD_ENDPOINT`, `CONFLUENT_API_KEY`, and `CONFLUENT_API_SECRET`

----
kafka.bootstrap.servers: <<CONFLUENT_CLOUD_ENDPOINT_HERE>>
kafka.sasl.jaas.config: org.apache.kafka.common.security.plain.PlainLoginModule required username="<<CONFLUENT_API_KEY HERE>>" password="<<CONFLUENT_API_SECRET HERE>>";
kafka.ssl.endpoint.identification.algorithm: https
kafka.security.protocol: SASL_SSL
kafka.sasl.mechanism: PLAIN
kafka.request.timeout_ms: 20000
kafka.retry.backoff.ms: 500
----

4. Decide if you want to save data to Neo4j using the plugin, produce data from Neo4j, or both
and then follow these extra steps accordingly:

5. (Optional) Take data from Kafka and store it in Neo4j (Neo4j as a data sink) by adding configuration such as:

.neo4j.conf
[source,ini]
----
streams.sink.enabled=true
streams.sink.topic.cypher.my-ingest-topic=MERGE (n:Label {id: event.id}) ON CREATE SET n += event.properties
----

This will process every message that comes in on `my-ingest-topic` with the given cypher statement.  When
that cypher statement executes, the `event` variable that is referenced will be set to the message received,
so this sample cypher will create a `(:Label)` node in the graph with the given ID, copying all of the
properties in the source message.

For full details on what you can do here, see the link:/consumer[Consumer Section] of the documentation.

6. (Optional) Produce data from Neo4j and send it to a Kafka topic (Neo4j as a source) by adding configuration such as:

.neo4j.conf
----
streams.source.topic.nodes.my-nodes-topic=Person{*}
streams.source.topic.relationships.my-rels-topic=KNOWS{*}
streams.source.enabled=true
streams.source.schema.polling.interval=10000
----

This will produce all graph nodes labeled `(:Person)` on to the topic `my-nodes-topic` and all
relationships of type `-[:KNOWS]->` to the topic named `my-rels-topic`.  Further, schema changes will
be polled every 10,000 ms, which affects how quickly the database picks up new indexes/schema changes.

The expressions `Person{\*}` and `KNOWS{*}` are _patterns_.  You can find documentation on how to change
these in the link:/producer/#_patterns[Patterns section].

For full details on what you can do here, see the link:/producer[Producer Section] of the documentation.

[[introduction]]
== Introduction

ifdef::env-docs[]
[abstract]
--
This chapter provides an introduction to the Neo4j Streams Library, and instructions for installation.
--
endif::env-docs[]

Many user and customers want to integrate Kafka and other streaming solutions with Neo4j.
Either to ingest data into the graph from other sources.
Or to send update events (change data capture - CDC) to the event log for later consumption.

This extension was developed to satisfy all these use-cases and more to come.

The project is composed of several parts:

* Neo4j Streams Procedure: a procedure to send a payload to a topic
* Neo4j Streams Producer: a transaction event handler events that sends data to a Kafka topic
* Neo4j Streams Consumer: a Neo4j application that ingest data from Kafka topics into Neo4j via templated Cypher Statements
* Kafka-Connect Plugin: a plugin for the Confluent Platform that allows to ingest data into Neo4j, from Kafka topics, via Cypher queries.

[[installation]]
=== Installation

Download the latest release jar from https://github.com/neo4j-contrib/neo4j-streams/releases/latest

Copy it into `$NEO4J_HOME/plugins` and configure the relevant connections.

[[configuration]]
=== Configuration

Configuring neo4j-streams comes in three different parts, depending on your need:

. *Required*: Configuring a connection to Kafka
. _Optional_: Configuring Neo4j to ingest from Kafka (link:/consumer[Consumer])
. _Optional_: Configuring Neo4j to produce records to Kafka (link:/producer[Producer])

This section will deal with configuring the plugin's connection to Kafka.  See the links above
in the sub-sections for link:/consumer[Consumer] and link:/producer[Producer] for documentation
on how to configure those aspects.

==== Kafka Configuration Settings

Neo4j streams uses the official Confluent Kafka producer and consumer java clients.  This means that
configuration settings which are valid for those connectors will also work for Neo4j Streams.  Any
configuration option that starts with `kafka.` will be passed to the driver.  For example, in the
kafka documentation linked below, the configuration setting named `batch.size` should be stated as
`kafka.batch.size` in Neo4j Streams.

The following are a couple of common configuration settings you may wish to use.  _This is not a complete
list_.  See the references at the bottom of this section for a full reference table of available configuration
options.

.Common Configuration Settings
|===
|Setting Name |Description |Default Value

|kafka.max.poll.records
|The maximum number of records to pull per batch from Kafka. Increasing this number will mean
larger transactions in Neo4j memory and may improve throughput.
|500

|Cell in column 1, row 2
|Cell in column 2, row 2
|Cell in column 3, row 2
|===

Important further configuration references:
* link:https://docs.confluent.io/current/installation/configuration/consumer-configs.html#cp-config-consumer[Consumer Configurations]
* link:https://docs.confluent.io/current/installation/configuration/producer-configs.html#cp-config-producer[Producer Configurations]

==== When Running Neo4j in Docker

When Neo4j is run in a docker, some special considerations apply; please see 
link:https://neo4j.com/docs/operations-manual/current/docker/configuration/[Neo4j Docker Configuration]
for more information.  In particular, the configuration format used in `neo4j.conf` looks like this:

----
kafka.zookeeper.connect=localhost:2181
kafka.bootstrap.servers=broker:9093
----

Where the same configuration for Neo4j running in docker would be done by environment variables,
like this:

----
NEO4J_kafka_zookeeper_connect: zookeeper:2181
NEO4J_kafka_bootstrap_servers: broker:9093
----

Note the configuration style that you need, and adapt examples in this documentation accordingly.

include::producer/index.adoc[]

include::consumer/index.adoc[]

include::procedures/index.adoc[]

include::docker/index.adoc[]

include::kafka-connect/index.adoc[]

include::developing/index.adoc[]